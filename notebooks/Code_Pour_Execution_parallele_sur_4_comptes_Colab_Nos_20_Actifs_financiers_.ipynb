{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAYdEzHo8fZV",
        "outputId": "94ff6f07-cf33-46ef-e31e-b337dbff2015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arch\n",
            "  Downloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from arch) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from arch) (1.15.3)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.11/dist-packages (from arch) (2.2.2)\n",
            "Requirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.11/dist-packages (from arch) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2025.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.12->arch) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.12->arch) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->arch) (1.17.0)\n",
            "Downloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (985 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/985.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m985.3/985.3 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: arch\n",
            "Successfully installed arch-7.2.0\n",
            "\n",
            "--- Machine 1 starting analysis for: ['AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN'] ---\n",
            "\n",
            "============================== Analyzing AAPL ==============================\n",
            "\n",
            "--- AAPL: Fetching and Preparing Data ---\n",
            "YF.download() has changed argument auto_adjust default to True\n",
            "Error processing data for AAPL: 'Adj Close'\n",
            "\n",
            "============================== Analyzing MSFT ==============================\n",
            "\n",
            "--- MSFT: Fetching and Preparing Data ---\n",
            "Error processing data for MSFT: 'Adj Close'\n",
            "\n",
            "============================== Analyzing GOOGL ==============================\n",
            "\n",
            "--- GOOGL: Fetching and Preparing Data ---\n",
            "Error processing data for GOOGL: 'Adj Close'\n",
            "\n",
            "============================== Analyzing GOOG ==============================\n",
            "\n",
            "--- GOOG: Fetching and Preparing Data ---\n",
            "Error processing data for GOOG: 'Adj Close'\n",
            "\n",
            "============================== Analyzing AMZN ==============================\n",
            "\n",
            "--- AMZN: Fetching and Preparing Data ---\n",
            "Error processing data for AMZN: 'Adj Close'\n",
            "\n",
            "No results generated for Machine 1.\n",
            "--- Machine 1 finished ---\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL A: Main Analysis Function Definition (Complete)\n",
        "# ==============================================================================\n",
        "!pip install arch\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from arch import arch_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import os # For creating directory\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create directory for convergence plots if it doesn't exist\n",
        "if not os.path.exists('convergence_plots'):\n",
        "    os.makedirs('convergence_plots')\n",
        "\n",
        "def create_lstm_sequences_global(features_data, target_data, n_steps_in):\n",
        "    X, y = [], []\n",
        "    # Ensure target_data is long enough relative to features_data and n_steps_in\n",
        "    if len(target_data) < n_steps_in or len(features_data) < n_steps_in : # Should not happen if called correctly\n",
        "        return np.array(X), np.array(y)\n",
        "    for i in range(n_steps_in, len(features_data) + 1): # Iterate up to the end of features\n",
        "        # The sequence of features is from i-n_steps_in to i-1\n",
        "        # The target y corresponds to the state at time i (or features ending at i-1 predict y at i)\n",
        "        # Here, target_data is indexed such that target_data[k] corresponds to features_data[k]\n",
        "        # So, if features_data[i-n_steps_in : i] are features for y at time i (original indexing)\n",
        "        # then target_data needs to be indexed at `i-1` if target_data is 0-indexed from features_data\n",
        "        if i > len(target_data): # Ensure we don't go out of bounds for target\n",
        "            break\n",
        "        X.append(features_data[i-n_steps_in:i, :])\n",
        "        y.append(target_data[i-1]) # target_data[i-1] aligns with features ending at index i-1\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "def analyze_single_stock(ticker_symbol, start_date, end_date_main_analysis, end_date_validation):\n",
        "    \"\"\"\n",
        "    Performs the full volatility modeling pipeline for a single stock.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30} Analyzing {ticker_symbol} {'='*30}\")\n",
        "    TICKER = ticker_symbol\n",
        "\n",
        "    current_stock_results_list = []\n",
        "    # These are for storing predictions of the *current stock* if you wanted to plot its best model here\n",
        "    # _local_all_test_predictions = {}\n",
        "    # _local_all_test_true_values = {}\n",
        "\n",
        "\n",
        "    # --- Cell 2 & 3: Data Fetching, Cleaning, Feature Engineering ---\n",
        "    print(f\"\\n--- {TICKER}: Fetching and Preparing Data ---\")\n",
        "    try:\n",
        "        data_full = yf.download(TICKER, start=start_date, end=end_date_validation, progress=False)\n",
        "        if data_full.empty:\n",
        "            print(f\"No data for {TICKER}. Skipping.\")\n",
        "            return pd.DataFrame(), {}\n",
        "\n",
        "        data_main = data_full[:end_date_main_analysis].copy()\n",
        "        data_val = data_full[end_date_main_analysis:end_date_validation].copy()\n",
        "        if not data_val.empty and not data_main.empty and data_val.index[0] <= data_main.index[-1]:\n",
        "             data_val = data_val[data_val.index > data_main.index[-1]]\n",
        "\n",
        "        if data_main.empty:\n",
        "            print(f\"No main analysis data for {TICKER} up to {end_date_main_analysis}. Skipping.\")\n",
        "            return pd.DataFrame(), {}\n",
        "\n",
        "        price_series_main = data_main['Adj Close'].copy()\n",
        "        if isinstance(price_series_main, pd.DataFrame):\n",
        "            price_series_main = price_series_main.iloc[:,0]\n",
        "        price_series_main.dropna(inplace=True)\n",
        "        if len(price_series_main) < 2: print(f\"Too little price data for {TICKER}\"); return pd.DataFrame(), {}\n",
        "\n",
        "        log_returns_main = (np.log(price_series_main) - np.log(price_series_main.shift(1))).dropna()\n",
        "        log_returns_main.name = 'Log_Returns'\n",
        "        VOL_WINDOW = 30 # Defined here\n",
        "        if len(log_returns_main) < VOL_WINDOW: print(f\"Too little return data for {TICKER} for VOL_WINDOW {VOL_WINDOW}\"); return pd.DataFrame(), {}\n",
        "\n",
        "        realized_vol_main = log_returns_main.rolling(window=VOL_WINDOW).std() * np.sqrt(252)\n",
        "        realized_vol_main.name = 'Realized_Vol'\n",
        "\n",
        "        df_model = pd.DataFrame({'Log_Returns': log_returns_main, 'Realized_Vol': realized_vol_main}).dropna()\n",
        "        if df_model.empty:\n",
        "            print(f\"df_model is empty for {TICKER} after processing main data. Skipping.\")\n",
        "            return pd.DataFrame(), {}\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data for {TICKER}: {e}\")\n",
        "        return pd.DataFrame(), {}\n",
        "\n",
        "    # --- Cell 4: Evaluation Function and Train/Test Split ---\n",
        "    print(f\"\\n--- {TICKER}: Setting up Evaluation and Train/Test Split ---\")\n",
        "    def _evaluate_model_local(y_true, y_pred, model_name=\"Model\"): # Removed store_preds\n",
        "        y_true_eval = np.array(y_true).flatten()\n",
        "        y_pred_eval = np.array(y_pred).flatten()\n",
        "        min_len = min(len(y_true_eval), len(y_pred_eval))\n",
        "        if min_len == 0: return {'Model': model_name, 'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan, 'Ticker': TICKER}\n",
        "        y_true_eval, y_pred_eval = y_true_eval[:min_len], y_pred_eval[:min_len]\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(y_true_eval, y_pred_eval))\n",
        "        mae = mean_absolute_error(y_true_eval, y_pred_eval)\n",
        "        r2 = r2_score(y_true_eval, y_pred_eval)\n",
        "        return {'Model': model_name, 'RMSE': rmse, 'MAE': mae, 'R2': r2, 'Ticker': TICKER}\n",
        "\n",
        "    test_size_fraction = 0.2\n",
        "    split_idx = int(len(df_model) * (1 - test_size_fraction))\n",
        "    if split_idx == 0 and len(df_model) > 1: split_idx = 1\n",
        "    elif split_idx == len(df_model) and len(df_model) > 1: split_idx = len(df_model)-1\n",
        "    elif len(df_model) < 2 : print(f\"Cannot split {TICKER}\"); return pd.DataFrame(), {}\n",
        "\n",
        "    garch_train_returns = df_model['Log_Returns'][:split_idx].copy() * 100\n",
        "    garch_test_returns = df_model['Log_Returns'][split_idx:].copy() * 100\n",
        "    garch_test_realized_vol = df_model['Realized_Vol'][split_idx:].copy()\n",
        "\n",
        "    ml_train_target = df_model['Realized_Vol'][:split_idx].copy()\n",
        "    ml_test_target = df_model['Realized_Vol'][split_idx:].copy()\n",
        "\n",
        "    if garch_test_returns.empty or garch_test_realized_vol.empty :\n",
        "        print(f\"GARCH test set empty for {TICKER}. GARCH models will have no test data.\")\n",
        "        # Allow to continue for ML if ML data is fine\n",
        "\n",
        "    # --- Cell 5: GARCH Models ---\n",
        "    print(f\"\\n--- {TICKER}: Fitting GARCH-Family Models ---\")\n",
        "    garch_predictions_main = {}\n",
        "    garch_results_objects_main = {} # Store fitted GARCH objects for in-sample vol for hybrids\n",
        "    DISTRIBUTIONS = ['normal', 't', 'ged']\n",
        "    HORIZON_LENGTH_MAIN = len(garch_test_returns) if not garch_test_returns.empty else 0\n",
        "\n",
        "    garch_model_specs = {\n",
        "        \"GARCH(1,1)\": {'vol': 'Garch', 'p': 1, 'o': 0, 'q': 1},\n",
        "        \"EGARCH(1,1)\": {'vol': 'EGARCH', 'p': 1, 'o': 1, 'q': 1},\n",
        "        \"APARCH(1,1)\": {'vol': 'APARCH', 'p': 1, 'o': 1, 'q': 1}, # delta estimated by default\n",
        "        \"GJR-GARCH(1,1)\": {'vol': 'Garch', 'p': 1, 'o': 1, 'q': 1}\n",
        "    }\n",
        "\n",
        "    if not garch_train_returns.empty and HORIZON_LENGTH_MAIN > 0:\n",
        "        for model_base_name, params in garch_model_specs.items():\n",
        "            for dist_name in DISTRIBUTIONS:\n",
        "                model_label = f\"{model_base_name}_{dist_name}\"\n",
        "                # print(f\"  Fitting {model_label} for {TICKER}...\") # Verbose\n",
        "                try:\n",
        "                    garch_m = arch_model(garch_train_returns, dist=dist_name, **params).fit(disp='off', show_warning=False)\n",
        "                    garch_results_objects_main[model_label] = garch_m # Store for hybrid features\n",
        "\n",
        "                    forecast_method_garch = 'simulation' if params['vol'] in ['EGARCH', 'APARCH'] else 'analytic'\n",
        "                    if forecast_method_garch == 'simulation':\n",
        "                        fc = garch_m.forecast(horizon=HORIZON_LENGTH_MAIN, method='simulation', simulations=500, reindex=False) # Reduced simulations\n",
        "                    else:\n",
        "                        fc = garch_m.forecast(horizon=HORIZON_LENGTH_MAIN, reindex=False)\n",
        "\n",
        "                    pred_var = fc.variance.values.flatten()\n",
        "                    if len(pred_var) != HORIZON_LENGTH_MAIN: pred_var = np.full(HORIZON_LENGTH_MAIN, np.nan) # Fallback\n",
        "\n",
        "                    pred_vol = np.sqrt(pred_var)/100 * np.sqrt(252)\n",
        "                    garch_predictions_main[model_label] = pred_vol\n",
        "                    current_stock_results_list.append(_evaluate_model_local(garch_test_realized_vol, pred_vol, model_label))\n",
        "                except Exception as e:\n",
        "                    # print(f\"    Error GARCH {model_label} for {TICKER}: {e}\")\n",
        "                    garch_predictions_main[model_label] = np.full(HORIZON_LENGTH_MAIN, np.nan)\n",
        "                    garch_results_objects_main[model_label] = None # Indicate failure\n",
        "    else:\n",
        "        print(f\"  Skipping GARCH fitting for {TICKER} due to empty train returns or zero horizon.\")\n",
        "\n",
        "\n",
        "    garch_preds_for_hybrid_main = pd.DataFrame(index=ml_test_target.index if not ml_test_target.empty else None)\n",
        "    for model_name_key, preds_val in garch_predictions_main.items():\n",
        "        col_hybrid_name = f'{model_name_key}_Pred_Vol'\n",
        "        if isinstance(preds_val, np.ndarray) and preds_val.ndim == 1 and len(preds_val) == len(garch_preds_for_hybrid_main):\n",
        "            garch_preds_for_hybrid_main[col_hybrid_name] = preds_val\n",
        "        else: # Ensure column exists even if preds failed or length mismatch\n",
        "            garch_preds_for_hybrid_main[col_hybrid_name] = np.nan\n",
        "\n",
        "\n",
        "    # --- Cell 6: ML Data Prep ---\n",
        "    print(f\"\\n--- {TICKER}: Preparing Data for ML Models ---\")\n",
        "    N_LAGS = 10\n",
        "    df_ml = df_model.copy()\n",
        "    for i in range(1, N_LAGS + 1):\n",
        "        df_ml[f'Log_Returns_Lag_{i}'] = df_ml['Log_Returns'].shift(i)\n",
        "        df_ml[f'Realized_Vol_Lag_{i}'] = df_ml['Realized_Vol'].shift(i)\n",
        "    df_ml.dropna(inplace=True)\n",
        "\n",
        "    SKIP_ML_THIS_STOCK = False\n",
        "    if df_ml.empty or len(df_ml) < N_LAGS + 5 :\n",
        "        print(f\"  Not enough data for ML for {TICKER} after lagging. Skipping ML & Hybrid models.\")\n",
        "        SKIP_ML_THIS_STOCK = True\n",
        "\n",
        "    if not SKIP_ML_THIS_STOCK:\n",
        "        y_ml = df_ml['Realized_Vol'].copy()\n",
        "        X_ml = df_ml.drop(['Log_Returns', 'Realized_Vol'], axis=1).copy()\n",
        "        split_idx_ml = int(len(X_ml) * (1 - test_size_fraction))\n",
        "        if split_idx_ml == 0 and len(X_ml) > 1 : split_idx_ml = 1\n",
        "        elif split_idx_ml == len(X_ml) and len(X_ml) > 1 : split_idx_ml = len(X_ml)-1\n",
        "        elif len(X_ml) < 2: SKIP_ML_THIS_STOCK = True; print(f\"Skipping ML for {TICKER} due to insufficient data for split after lag.\")\n",
        "\n",
        "    if not SKIP_ML_THIS_STOCK:\n",
        "        X_train_ml = X_ml[:split_idx_ml].copy()\n",
        "        X_test_ml = X_ml[split_idx_ml:].copy()\n",
        "        y_train_ml = y_ml[:split_idx_ml].copy()\n",
        "        y_test_ml = y_ml[split_idx_ml:].copy()\n",
        "\n",
        "        if y_train_ml.empty or y_test_ml.empty: # Further check\n",
        "            print(f\"  ML train or test target is empty for {TICKER}. Skipping ML & Hybrid.\")\n",
        "            SKIP_ML_THIS_STOCK = True\n",
        "        else:\n",
        "            scaler_X = MinMaxScaler()\n",
        "            X_train_ml_scaled = scaler_X.fit_transform(X_train_ml)\n",
        "            X_test_ml_scaled = scaler_X.transform(X_test_ml)\n",
        "\n",
        "            # --- Cells 7-10: Standalone ML Models ---\n",
        "            print(f\"\\n--- {TICKER}: Fitting Standalone ML Models ---\")\n",
        "            ml_models_to_run = {\n",
        "                \"XGBoost\": xgb.XGBRegressor(objective='reg:squarederror', n_estimators=50, random_state=42, early_stopping_rounds=5),\n",
        "                \"Random Forest\": RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1),\n",
        "                \"DFFNN\": Sequential([Dense(64, activation='relu', input_shape=(X_train_ml_scaled.shape[1],)), Dropout(0.2), Dense(32, activation='relu'), Dense(1)]),\n",
        "                \"LSTM\": Sequential([LSTM(32, activation='relu', input_shape=(N_STEPS_LSTM, X_ml_scaled_for_lstm_seq.shape[1] if 'X_ml_scaled_for_lstm_seq' in locals() and X_ml_scaled_for_lstm_seq.ndim==2 else X_train_ml_scaled.shape[1] // N_LAGS if N_LAGS > 0 else 2 ), return_sequences=False), Dropout(0.2), Dense(16, activation='relu'), Dense(1)]) # Simplified LSTM input shape guess\n",
        "            }\n",
        "            N_STEPS_LSTM = 5 # Define for LSTM\n",
        "\n",
        "            for ml_name, ml_model_obj in ml_models_to_run.items():\n",
        "                # print(f\"    Fitting {ml_name} for {TICKER}...\") # Verbose\n",
        "                try:\n",
        "                    if ml_name in [\"XGBoost\", \"Random Forest\"]:\n",
        "                        ml_model_obj.fit(X_train_ml, y_train_ml, eval_set=[(X_test_ml, y_test_ml)], verbose=False) if ml_name == \"XGBoost\" else ml_model_obj.fit(X_train_ml, y_train_ml)\n",
        "                        preds = ml_model_obj.predict(X_test_ml)\n",
        "                        current_stock_results_list.append(_evaluate_model_local(y_test_ml, preds, ml_name))\n",
        "                    elif ml_name == \"DFFNN\":\n",
        "                        ml_model_obj.compile(optimizer='adam', loss='mean_squared_error')\n",
        "                        es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "                        hist = ml_model_obj.fit(X_train_ml_scaled, y_train_ml, epochs=50, batch_size=32, validation_split=0.1, callbacks=[es], verbose=0)\n",
        "                        plt.figure(figsize=(7,4)); plt.plot(hist.history['loss'], label='Train'); plt.plot(hist.history['val_loss'], label='Val'); plt.title(f'{ml_name} Conv - {TICKER}'); plt.legend(); plt.savefig(f'convergence_plots/{TICKER}_{ml_name}_convergence.png'); plt.close()\n",
        "                        preds = ml_model_obj.predict(X_test_ml_scaled).flatten()\n",
        "                        current_stock_results_list.append(_evaluate_model_local(y_test_ml, preds, ml_name))\n",
        "                    elif ml_name == \"LSTM\":\n",
        "                        # LSTM Data Prep (simplified using all X_ml features, reshape for sequence)\n",
        "                        scaler_lstm_feat_standalone = MinMaxScaler()\n",
        "                        X_ml_scaled_for_lstm_seq = scaler_lstm_feat_standalone.fit_transform(X_ml)\n",
        "\n",
        "                        X_lstm_seq_train, y_lstm_seq_train = create_lstm_sequences_global(X_ml_scaled_for_lstm_seq[:split_idx_ml], y_ml.values[:split_idx_ml], N_STEPS_LSTM)\n",
        "                        X_lstm_seq_test, y_lstm_seq_test = create_lstm_sequences_global(X_ml_scaled_for_lstm_seq[split_idx_ml:], y_ml.values[split_idx_ml:], N_STEPS_LSTM)\n",
        "\n",
        "                        if X_lstm_seq_train.size > 0 and X_lstm_seq_test.size > 0:\n",
        "                            # Re-define LSTM with correct input_shape if first time\n",
        "                            if not ml_model_obj.built: # Check if model is already compiled/built\n",
        "                                ml_model_obj = Sequential([LSTM(32, activation='relu', input_shape=(N_STEPS_LSTM, X_lstm_seq_train.shape[2]), return_sequences=False), Dropout(0.2), Dense(16, activation='relu'), Dense(1)])\n",
        "                            ml_model_obj.compile(optimizer='adam', loss='mean_squared_error')\n",
        "                            es_lstm = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "                            hist_lstm = ml_model_obj.fit(X_lstm_seq_train, y_lstm_seq_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[es_lstm], verbose=0)\n",
        "                            plt.figure(figsize=(7,4)); plt.plot(hist_lstm.history['loss'], label='Train'); plt.plot(hist_lstm.history['val_loss'], label='Val'); plt.title(f'{ml_name} Conv - {TICKER}'); plt.legend(); plt.savefig(f'convergence_plots/{TICKER}_{ml_name}_convergence.png'); plt.close()\n",
        "                            preds = ml_model_obj.predict(X_lstm_seq_test).flatten()\n",
        "                            current_stock_results_list.append(_evaluate_model_local(y_lstm_seq_test, preds, ml_name))\n",
        "                        else:\n",
        "                            print(f\"    Skipping LSTM for {TICKER} due to insufficient sequence data.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"    Error {ml_name} for {TICKER}: {e}\")\n",
        "\n",
        "\n",
        "            # --- Cell 11: Hybrid Models ---\n",
        "            print(f\"\\n--- {TICKER}: Fitting Hybrid Models ---\")\n",
        "            garch_features_for_train_hybrid_stock = pd.DataFrame(index=X_train_ml.index)\n",
        "            if not garch_train_returns.empty:\n",
        "                for model_lbl, garch_obj_fit in garch_results_objects_main.items():\n",
        "                    col_name_hyb_train = f\"{model_lbl}_Pred_Vol\"\n",
        "                    if garch_obj_fit is not None: # Check if model was successfully fitted\n",
        "                        try:\n",
        "                            cond_vol = garch_obj_fit.conditional_volatility / 100 * np.sqrt(252)\n",
        "                            garch_features_for_train_hybrid_stock[col_name_hyb_train] = cond_vol.reindex(X_train_ml.index).fillna(method='bfill').fillna(method='ffill')\n",
        "                        except Exception: # Broad except for any issue with conditional_volatility\n",
        "                            garch_features_for_train_hybrid_stock[col_name_hyb_train] = 0 # Fallback\n",
        "                    else: # Model fitting failed earlier\n",
        "                        garch_features_for_train_hybrid_stock[col_name_hyb_train] = 0\n",
        "\n",
        "            # Ensure all GARCH pred columns from test set exist in train features (even if as 0)\n",
        "            for expected_garch_col in garch_preds_for_hybrid_main.columns:\n",
        "                if expected_garch_col not in garch_features_for_train_hybrid_stock.columns:\n",
        "                    garch_features_for_train_hybrid_stock[expected_garch_col] = 0\n",
        "\n",
        "\n",
        "            X_train_hybrid_base_s = X_train_ml.join(garch_features_for_train_hybrid_stock, how='left').fillna(0)\n",
        "            X_test_hybrid_base_s = X_test_ml.join(garch_preds_for_hybrid_main, how='left').fillna(0)\n",
        "\n",
        "            common_hybrid_cols_s = list(X_test_ml.columns) + list(garch_preds_for_hybrid_main.columns) # Order: base ML, then GARCH\n",
        "            for col_s in common_hybrid_cols_s:\n",
        "                if col_s not in X_train_hybrid_base_s.columns: X_train_hybrid_base_s[col_s] = 0\n",
        "                if col_s not in X_test_hybrid_base_s.columns: X_test_hybrid_base_s[col_s] = 0\n",
        "\n",
        "            X_train_hybrid_final_s = X_train_hybrid_base_s[common_hybrid_cols_s].copy()\n",
        "            X_test_hybrid_final_s = X_test_hybrid_base_s[common_hybrid_cols_s].copy()\n",
        "\n",
        "            base_ml_feature_cols_s = list(X_train_ml.columns)\n",
        "            for specific_garch_feat_col in garch_preds_for_hybrid_main.columns: # Iterate over the 12 GARCH feature column names\n",
        "                # print(f\"  Hybridizing with: {specific_garch_feat_col}\") # Verbose\n",
        "                current_hybrid_cols = base_ml_feature_cols_s + [specific_garch_feat_col]\n",
        "\n",
        "                curr_X_train_hyb = X_train_hybrid_final_s[current_hybrid_cols]\n",
        "                curr_X_test_hyb = X_test_hybrid_final_s[current_hybrid_cols]\n",
        "\n",
        "                scaler_hyb = MinMaxScaler()\n",
        "                curr_X_train_hyb_sc = scaler_hyb.fit_transform(curr_X_train_hyb)\n",
        "                curr_X_test_hyb_sc = scaler_hyb.transform(curr_X_test_hyb)\n",
        "\n",
        "                garch_comp_name_for_label = specific_garch_feat_col.replace('_Pred_Vol','')\n",
        "\n",
        "                for ml_name, ml_model_obj_hyb_template in ml_models_to_run.items(): # Reuse templates\n",
        "                    model_name_hyb = f\"{ml_name}_Hybrid_with_{garch_comp_name_for_label}\"\n",
        "                    # print(f\"    Fitting {model_name_hyb} for {TICKER}...\") # Verbose\n",
        "                    try:\n",
        "                        if ml_name in [\"XGBoost\", \"Random Forest\"]:\n",
        "                            ml_model_instance = ml_model_obj_hyb_template # Re-init for safety if needed\n",
        "                            ml_model_instance.fit(curr_X_train_hyb, y_train_ml, eval_set=[(curr_X_test_hyb, y_test_ml)], verbose=False) if ml_name == \"XGBoost\" else ml_model_instance.fit(curr_X_train_hyb, y_train_ml)\n",
        "                            preds_hyb = ml_model_instance.predict(curr_X_test_hyb)\n",
        "                            current_stock_results_list.append(_evaluate_model_local(y_test_ml, preds_hyb, model_name_hyb))\n",
        "                        elif ml_name == \"DFFNN\":\n",
        "                            dffnn_hyb_m = Sequential([Dense(64, activation='relu', input_shape=(curr_X_train_hyb_sc.shape[1],)), Dropout(0.2), Dense(32, activation='relu'), Dense(1)])\n",
        "                            dffnn_hyb_m.compile(optimizer='adam', loss='mean_squared_error')\n",
        "                            es_hyb_d = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "                            hist_d_hyb = dffnn_hyb_m.fit(curr_X_train_hyb_sc, y_train_ml, epochs=50, validation_split=0.1, callbacks=[es_hyb_d], verbose=0)\n",
        "                            plt.figure(figsize=(7,4)); plt.plot(hist_d_hyb.history['loss'], label='Train'); plt.plot(hist_d_hyb.history['val_loss'], label='Val'); plt.title(f'{model_name_hyb} Conv - {TICKER}'); plt.legend(); plt.savefig(f'convergence_plots/{TICKER}_{model_name_hyb}_convergence.png'); plt.close()\n",
        "                            preds_hyb = dffnn_hyb_m.predict(curr_X_test_hyb_sc).flatten()\n",
        "                            current_stock_results_list.append(_evaluate_model_local(y_test_ml, preds_hyb, model_name_hyb))\n",
        "                        elif ml_name == \"LSTM\":\n",
        "                            X_lstm_h_train_seq, y_lstm_h_train_seq = create_lstm_sequences_global(curr_X_train_hyb_sc, y_train_ml.values, N_STEPS_LSTM)\n",
        "                            X_lstm_h_test_seq, y_lstm_h_test_seq = create_lstm_sequences_global(curr_X_test_hyb_sc, y_test_ml.values, N_STEPS_LSTM)\n",
        "                            if X_lstm_h_train_seq.size > 0 and X_lstm_h_test_seq.size > 0:\n",
        "                                lstm_hyb_m = Sequential([LSTM(32, activation='relu', input_shape=(N_STEPS_LSTM, X_lstm_h_train_seq.shape[2]), return_sequences=False), Dropout(0.2), Dense(16, activation='relu'), Dense(1)])\n",
        "                                lstm_hyb_m.compile(optimizer='adam', loss='mean_squared_error')\n",
        "                                es_hyb_l = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "                                hist_l_hyb = lstm_hyb_m.fit(X_lstm_h_train_seq, y_lstm_h_train_seq, epochs=50, validation_split=0.1, callbacks=[es_hyb_l], verbose=0)\n",
        "                                plt.figure(figsize=(7,4)); plt.plot(hist_l_hyb.history['loss'], label='Train'); plt.plot(hist_l_hyb.history['val_loss'], label='Val'); plt.title(f'{model_name_hyb} Conv - {TICKER}'); plt.legend(); plt.savefig(f'convergence_plots/{TICKER}_{model_name_hyb}_convergence.png'); plt.close()\n",
        "                                preds_hyb = lstm_hyb_m.predict(X_lstm_h_test_seq).flatten()\n",
        "                                current_stock_results_list.append(_evaluate_model_local(y_lstm_h_test_seq, preds_hyb, model_name_hyb))\n",
        "                            else:\n",
        "                                print(f\"    Skipping {model_name_hyb} for {TICKER} due to insufficient sequence data.\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"    Error {model_name_hyb} for {TICKER}: {e}\")\n",
        "    else: # if SKIP_ML_THIS_STOCK was True\n",
        "        print(f\"  ML and Hybrid models skipped for {TICKER} due to earlier data issues.\")\n",
        "\n",
        "\n",
        "    # --- Short-Term Out-of-Sample Validation (Placeholder for full logic) ---\n",
        "    print(f\"\\n--- {TICKER}: Short-Term Validation (2 months) ---\")\n",
        "    best_model_details_for_validation = {'ticker': TICKER, 'best_model_for_validation': None, 'validation_RMSE': np.nan}\n",
        "    if current_stock_results_list:\n",
        "        temp_results_df_val = pd.DataFrame(current_stock_results_list)\n",
        "        if not temp_results_df_val.empty and 'RMSE' in temp_results_df_val.columns and temp_results_df_val['RMSE'].notna().any():\n",
        "            best_model_name_for_stock = temp_results_df_val.loc[temp_results_df_val['RMSE'].idxmin()]['Model']\n",
        "            print(f\"  Best model for {TICKER} (main analysis): {best_model_name_for_stock}\")\n",
        "            best_model_details_for_validation['best_model_for_validation'] = best_model_name_for_stock\n",
        "            # Actual validation forecasting logic is complex and omitted for this script's scope.\n",
        "            # It would involve refitting `best_model_name_for_stock` on `df_model`\n",
        "            # then predicting on features derived from `data_val`.\n",
        "            if not data_val.empty:\n",
        "                 print(f\"  (Validation on {len(data_val)} days of OOS data would be performed here if fully implemented)\")\n",
        "            else:\n",
        "                 print(f\"  No OOS data in data_val for {TICKER}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"  No valid RMSEs for {TICKER} to determine best model for validation.\")\n",
        "    else:\n",
        "        print(f\"  No results for {TICKER} to determine best model for validation.\")\n",
        "\n",
        "    final_df_for_stock = pd.DataFrame()\n",
        "    if current_stock_results_list:\n",
        "        final_df_for_stock = pd.DataFrame(current_stock_results_list)\n",
        "\n",
        "    print(f\"--- Finished {TICKER} ---\")\n",
        "    return final_df_for_stock, best_model_details_for_validation\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL B: Stock Lists and Main Execution Script\n",
        "# ==============================================================================\n",
        "\n",
        "MAIN_ANALYSIS_START_DATE = '2010-01-01'\n",
        "MAIN_ANALYSIS_END_DATE = '2022-12-31'\n",
        "VALIDATION_END_DATE = '2023-02-28' # Approx 2 months after main analysis\n",
        "\n",
        "# Top 20 US Stocks by Market Cap (example list, might change over time)\n",
        "ALL_US_STOCKS_FULL_LIST = [\n",
        "    'AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN', 'NVDA', 'TSLA', 'BRK-B', 'META', 'UNH',\n",
        "    'XOM', 'JNJ', 'JPM', 'V', 'LLY', 'PG', 'HD', 'MA', 'CVX', 'MRK'\n",
        "    # 'PEP', 'COST', 'AVGO', 'ABBV', 'ADBE' # More options\n",
        "]\n",
        "# Using first 20 for this example\n",
        "ALL_US_STOCKS = ALL_US_STOCKS_FULL_LIST[:20]\n",
        "\n",
        "\n",
        "num_stocks = len(ALL_US_STOCKS)\n",
        "stocks_per_machine = num_stocks // 4\n",
        "remain = num_stocks % 4\n",
        "\n",
        "machine_stock_lists = []\n",
        "current_idx = 0\n",
        "for i in range(4):\n",
        "    num_for_this_machine = stocks_per_machine + (1 if i < remain else 0)\n",
        "    machine_stock_lists.append(ALL_US_STOCKS[current_idx : current_idx + num_for_this_machine])\n",
        "    current_idx += num_for_this_machine\n",
        "\n",
        "# --- !!! THIS IS WHERE YOU SELECT WHICH MACHINE'S LIST TO RUN !!! ---\n",
        "# --- !!!      FOR EACH ACTUAL MACHINE, SET THIS MANUALLY        !!! ---\n",
        "MACHINE_TO_SIMULATE = 1 # CHANGE THIS FROM 1 to 4 for each \"machine\"\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "if 1 <= MACHINE_TO_SIMULATE <= 4:\n",
        "    STOCKS_TO_RUN_ON_THIS_MACHINE = machine_stock_lists[MACHINE_TO_SIMULATE - 1]\n",
        "    MACHINE_ID_STR = str(MACHINE_TO_SIMULATE)\n",
        "else:\n",
        "    print(\"Invalid MACHINE_TO_SIMULATE value. Set to 1, 2, 3, or 4.\")\n",
        "    STOCKS_TO_RUN_ON_THIS_MACHINE = [] # Avoid running if invalid\n",
        "    MACHINE_ID_STR = \"invalid\"\n",
        "\n",
        "\n",
        "print(f\"\\n--- Machine {MACHINE_ID_STR} starting analysis for: {STOCKS_TO_RUN_ON_THIS_MACHINE} ---\")\n",
        "\n",
        "all_results_machine = []\n",
        "all_validation_details_machine = []\n",
        "\n",
        "if STOCKS_TO_RUN_ON_THIS_MACHINE: # Only run if there are stocks assigned\n",
        "    for ticker_val in STOCKS_TO_RUN_ON_THIS_MACHINE:\n",
        "        stock_perf_df, stock_val_details_dict = analyze_single_stock(\n",
        "            ticker_symbol=ticker_val,\n",
        "            start_date=MAIN_ANALYSIS_START_DATE,\n",
        "            end_date_main_analysis=MAIN_ANALYSIS_END_DATE,\n",
        "            end_date_validation=VALIDATION_END_DATE\n",
        "        )\n",
        "        if not stock_perf_df.empty:\n",
        "            all_results_machine.append(stock_perf_df)\n",
        "        if stock_val_details_dict and stock_val_details_dict.get('best_model_for_validation') is not None: # Check if dict has content\n",
        "            all_validation_details_machine.append(stock_val_details_dict)\n",
        "\n",
        "\n",
        "    if all_results_machine:\n",
        "        combined_df_machine = pd.concat(all_results_machine, ignore_index=True)\n",
        "        output_filename = f'machine_{MACHINE_ID_STR}_stock_model_performance.csv'\n",
        "        combined_df_machine.to_csv(output_filename, index=False)\n",
        "        print(f\"\\nResults for Machine {MACHINE_ID_STR} saved to {output_filename}\")\n",
        "    else:\n",
        "        print(f\"\\nNo results generated for Machine {MACHINE_ID_STR}.\")\n",
        "\n",
        "    if all_validation_details_machine:\n",
        "       pd.DataFrame(all_validation_details_machine).to_csv(f'machine_{MACHINE_ID_STR}_validation_summary.csv', index=False)\n",
        "\n",
        "\n",
        "    print(f\"--- Machine {MACHINE_ID_STR} finished ---\")\n",
        "else:\n",
        "    print(f\"No stocks assigned to Machine {MACHINE_ID_STR}. Check MACHINE_TO_SIMULATE setting or stock list.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL C: Aggregation, Summary Table Generation, and Highlighting\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np # For NaN comparison if needed\n",
        "import glob # For finding files\n",
        "# For styled Excel output, you might need: pip install openpyxl\n",
        "# For displaying styled tables in Jupyter:\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "print(\"\\n--- Aggregating Results and Generating Summary Tables ---\")\n",
        "\n",
        "# --- Step 1: Load data from all machine CSVs ---\n",
        "all_machine_csv_files = sorted(glob.glob('machine_*_stock_model_performance.csv')) # Sort for consistent order\n",
        "\n",
        "if not all_machine_csv_files:\n",
        "    print(\"No machine CSV files found (e.g., 'machine_1_stock_model_performance.csv').\")\n",
        "    print(\"Please ensure the individual machine scripts have run successfully and produced output.\")\n",
        "    # Create an empty DataFrame to prevent errors later if no files are found\n",
        "    full_results_df = pd.DataFrame(columns=['Model', 'RMSE', 'MAE', 'R2', 'Ticker'])\n",
        "else:\n",
        "    print(f\"Found result files: {all_machine_csv_files}\")\n",
        "    list_of_dfs = []\n",
        "    for f in all_machine_csv_files:\n",
        "        try:\n",
        "            df_temp = pd.read_csv(f)\n",
        "            if not df_temp.empty:\n",
        "                list_of_dfs.append(df_temp)\n",
        "            else:\n",
        "                print(f\"Warning: File {f} is empty.\")\n",
        "        except pd.errors.EmptyDataError:\n",
        "            print(f\"Warning: File {f} is empty or malformed.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {f}: {e}\")\n",
        "\n",
        "    if list_of_dfs:\n",
        "        full_results_df = pd.concat(list_of_dfs, ignore_index=True)\n",
        "        print(f\"\\nSuccessfully aggregated {len(list_of_dfs)} result files.\")\n",
        "        print(f\"Total results loaded: {len(full_results_df)} model runs.\")\n",
        "    else:\n",
        "        print(\"No data loaded from any machine CSV files.\")\n",
        "        full_results_df = pd.DataFrame(columns=['Model', 'RMSE', 'MAE', 'R2', 'Ticker'])\n",
        "\n",
        "\n",
        "if full_results_df.empty:\n",
        "    print(\"Aggregated results DataFrame is empty. Cannot generate summaries.\")\n",
        "else:\n",
        "    print(\"\\n--- Full Aggregated Results (Sample) ---\")\n",
        "    display(HTML(full_results_df.head().to_html(index=False))) # Nicer display in Jupyter\n",
        "\n",
        "    # Drop rows where RMSE might be NaN (if a model failed entirely for a stock)\n",
        "    full_results_df.dropna(subset=['RMSE'], inplace=True)\n",
        "    if full_results_df.empty:\n",
        "        print(\"No valid RMSE values found after dropping NaNs. Cannot proceed.\")\n",
        "    else:\n",
        "        # --- Step 2: Find Best GARCH Model (including distribution) for each stock ---\n",
        "        # Ensure correct regex to capture only base GARCH models, not hybrid GARCH components if names are similar\n",
        "        garch_models_only_df = full_results_df[\n",
        "            full_results_df['Model'].str.match(r'^(GARCH\\(1,1\\)_|EGARCH\\(1,1\\)_|APARCH\\(1,1\\)_|GJR-GARCH\\(1,1\\)_)(normal|t|ged)$')\n",
        "        ].copy()\n",
        "\n",
        "        if not garch_models_only_df.empty:\n",
        "            best_garch_per_stock = garch_models_only_df.loc[garch_models_only_df.groupby('Ticker')['RMSE'].idxmin()]\n",
        "            best_garch_summary = best_garch_per_stock[['Ticker', 'Model', 'RMSE', 'MAE', 'R2']].rename(\n",
        "                columns={'Model': 'Best_GARCH_Variant'}\n",
        "            ).set_index('Ticker').sort_index()\n",
        "\n",
        "            print(\"\\n\\n--- Best GARCH Model (incl. Distribution) per Stock (by RMSE) ---\")\n",
        "            display(HTML(best_garch_summary.to_html()))\n",
        "            best_garch_summary.to_csv(\"summary_best_garch_model_per_stock.csv\")\n",
        "            print(\"Saved: summary_best_garch_model_per_stock.csv\")\n",
        "\n",
        "            # Create a pivot table for all GARCH models vs Tickers (RMSE)\n",
        "            pivot_garch_rmse = garch_models_only_df.pivot_table(index='Ticker', columns='Model', values='RMSE')\n",
        "\n",
        "            # Styling function to highlight the minimum RMSE in each row\n",
        "            def highlight_min_in_row(s):\n",
        "                is_min = s == s.min()\n",
        "                return ['background-color: yellow; font-weight: bold;' if v else '' for v in is_min]\n",
        "\n",
        "            styled_pivot_garch_rmse = pivot_garch_rmse.style.apply(highlight_min_in_row, axis=1)\\\n",
        "                                                            .format(precision=4)\\\n",
        "                                                            .set_caption(\"GARCH Models RMSE Comparison (Best per stock highlighted)\")\n",
        "            print(\"\\n\\n--- Styled GARCH Models RMSE Table (Best per stock highlighted) ---\")\n",
        "            display(styled_pivot_garch_rmse)\n",
        "            try:\n",
        "                styled_pivot_garch_rmse.to_excel(\"styled_garch_rmse_table.xlsx\", engine='openpyxl')\n",
        "                print(\"Saved: styled_garch_rmse_table.xlsx\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not save GARCH styled table to Excel (ensure openpyxl is installed): {e}\")\n",
        "        else:\n",
        "            print(\"\\nNo standalone GARCH model results found to determine best GARCH per stock.\")\n",
        "\n",
        "\n",
        "        # --- Step 3: Find Best Overall Model (Standalone or Hybrid) for each stock ---\n",
        "        best_overall_per_stock = full_results_df.loc[full_results_df.groupby('Ticker')['RMSE'].idxmin()]\n",
        "        best_overall_summary = best_overall_per_stock[['Ticker', 'Model', 'RMSE', 'MAE', 'R2']].rename(\n",
        "            columns={'Model': 'Best_Overall_Model'}\n",
        "        ).set_index('Ticker').sort_index()\n",
        "\n",
        "        print(\"\\n\\n--- Best Overall Model per Stock (by RMSE) ---\")\n",
        "        display(HTML(best_overall_summary.to_html()))\n",
        "        best_overall_summary.to_csv(\"summary_best_overall_model_per_stock.csv\")\n",
        "        print(\"Saved: summary_best_overall_model_per_stock.csv\")\n",
        "\n",
        "        # Optional: Create a styled pivot table for ALL models (can be very wide)\n",
        "        # Consider filtering to top N models or specific types if too large\n",
        "        try:\n",
        "            pivot_all_rmse = full_results_df.pivot_table(index='Ticker', columns='Model', values='RMSE')\n",
        "            styled_pivot_all_rmse = pivot_all_rmse.style.apply(highlight_min_in_row, axis=1)\\\n",
        "                                                        .format(precision=4)\\\n",
        "                                                        .set_caption(\"All Models RMSE Comparison (Best per stock highlighted)\")\n",
        "            print(\"\\n\\n--- Styled All Models RMSE Table (Best per stock highlighted) ---\")\n",
        "            print(\"(This table might be very wide)\")\n",
        "            # display(styled_pivot_all_rmse) # Can be too large for direct display\n",
        "            styled_pivot_all_rmse.to_excel(\"styled_all_models_rmse_table.xlsx\", engine='openpyxl')\n",
        "            print(\"Saved: styled_all_models_rmse_table.xlsx\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not create or save the full styled pivot table: {e}\")\n",
        "            print(\"This might be due to too many models making the table too wide for Excel or processing.\")\n",
        "\n",
        "print(\"\\n--- Aggregation and Summary Script Finished ---\")"
      ],
      "metadata": {
        "id": "XWx7ctdi9pwu",
        "outputId": "5da29f73-7132-428d-f161-e2753c97f2e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Aggregating Results and Generating Summary Tables ---\n",
            "No machine CSV files found (e.g., 'machine_1_stock_model_performance.csv').\n",
            "Please ensure the individual machine scripts have run successfully and produced output.\n",
            "Aggregated results DataFrame is empty. Cannot generate summaries.\n",
            "\n",
            "--- Aggregation and Summary Script Finished ---\n"
          ]
        }
      ]
    }
  ]
}